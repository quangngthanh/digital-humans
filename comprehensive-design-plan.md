# ğŸ¯ Thiáº¿t Káº¿ Tá»•ng Thá»ƒ - Digital Humans Expression System

## ğŸ“‹ **Tá»•ng Quan Kiáº¿n TrÃºc**

### **NguyÃªn Táº¯c Thiáº¿t Káº¿ Core**
- **AI quyáº¿t Ä‘á»‹nh Emotional Intent** (10-15 emotions cÆ¡ báº£n)
- **Frontend mapping thÃ´ng minh** â†’ Rich expressions (60+) + Animations
- **Separation of Concerns**: AI expert vá» ngÃ´n ngá»¯/tÃ¢m lÃ½, Frontend expert vá» visual
- **Scalable**: Dá»… thÃªm expressions/animations mÃ  khÃ´ng áº£nh hÆ°á»Ÿng AI

### **Kiáº¿n TrÃºc 3-Layer**
```
ğŸ§  AI Agent Layer     â†’ Emotional Intent + Context + Intensity
ğŸ”„ Intelligence Layer â†’ Context Analysis + Mapping Rules  
ğŸ­ Visual Layer       â†’ Expression Selection + Animation Coordination
```

---

## ğŸ—ï¸ **Kiáº¿n TrÃºc Chi Tiáº¿t**

### **Layer 1: AI Agent (Emotional Intent)**
**Responsibility**: Hiá»ƒu ngá»¯ cáº£nh vÃ  quyáº¿t Ä‘á»‹nh Ã½ Ä‘á»‹nh cáº£m xÃºc

```typescript
interface EmotionalIntent {
  primary: EmotionType;      // Cáº£m xÃºc chÃ­nh
  intensity: number;         // CÆ°á»ng Ä‘á»™ 0.1-1.0
  context: ContextType;      // Ngá»¯ cáº£nh cuá»™c trÃ³i chuyá»‡n
  duration: DurationType;    // Thá»i gian biá»ƒu hiá»‡n
}

enum EmotionType {
  // Basic emotions (Ekman + expanded)
  HAPPY = 'happy',
  SAD = 'sad', 
  EXCITED = 'excited',
  ROMANTIC = 'romantic',
  PLAYFUL = 'playful',
  SERIOUS = 'serious',
  SURPRISED = 'surprised',
  CONFUSED = 'confused',
  CARING = 'caring',
  MISCHIEVOUS = 'mischievous',
  THOUGHTFUL = 'thoughtful',
  CONFIDENT = 'confident',
  SHY = 'shy',
  FRUSTRATED = 'frustrated',
  CURIOUS = 'curious'
}
```

### **Layer 2: Intelligence Layer (Context Analysis)**
**Responsibility**: PhÃ¢n tÃ­ch context vÃ  Ã¡p dá»¥ng mapping rules

```typescript
interface ConversationContext {
  messageType: 'greeting' | 'question' | 'compliment' | 'story' | 'goodbye';
  relationshipTone: 'casual' | 'romantic' | 'playful' | 'intimate' | 'supportive';
  conversationMood: 'light' | 'deep' | 'flirty' | 'serious' | 'comfort';
  userEmotionalState: 'positive' | 'negative' | 'neutral' | 'mixed';
}

interface MappingRules {
  expressionGroups: ExpressionGroupMap;
  animationGroups: AnimationGroupMap; 
  transitionRules: TransitionRuleMap;
  contextOverrides: ContextOverrideMap;
}
```

### **Layer 3: Visual Layer (Expression + Animation)**
**Responsibility**: Render biá»ƒu cáº£m vÃ  animation phÃ¹ há»£p

```typescript
interface AvatarState {
  expression: ExpressionName;
  animation: AnimationName;
  transitionDuration: number;
  blendingRatio?: number;  // Cho complex expressions
}
```

---

## ğŸ“Š **Data Models**

### **AI Response Format**
```typescript
interface AIMessage {
  text: string;
  emotionalIntent: {
    primary: EmotionType;
    intensity: number;        // 0.1 - 1.0
    context: ContextType;
    secondaryEmotion?: EmotionType;  // Cho mixed emotions
  };
  metadata: {
    messageLength: number;
    estimatedDuration: number;
    conversationTurn: number;
  };
}
```

### **Expression Mapping System**
```typescript
interface ExpressionGroup {
  category: 'basic' | 'complex' | 'social' | 'cognitive' | 'micro';
  intensityLevels: {
    low: ExpressionName[];
    medium: ExpressionName[];
    high: ExpressionName[];
  };
  contextualVariants: {
    [context: string]: ExpressionName[];
  };
  animationCompatibility: AnimationName[];
}

// Example:
const HAPPY_GROUP: ExpressionGroup = {
  category: 'basic',
  intensityLevels: {
    low: ['smile', 'content'],
    medium: ['joy', 'cheerful'],
    high: ['euphoric', 'laughing']
  },
  contextualVariants: {
    romantic: ['flirtatious', 'loving'],
    playful: ['mischievous', 'cheeky'],
    casual: ['smile', 'welcoming']
  },
  animationCompatibility: ['Talking_1', 'Talking_2', 'Laughing', 'Heart_Gesture']
};
```

### **Animation Coordination System**
```typescript
interface AnimationGroup {
  category: 'conversation' | 'emotional' | 'social' | 'reactive';
  duration: 'short' | 'medium' | 'long' | 'loop';
  emotionCompatibility: EmotionType[];
  transitionFrom: AnimationName[];
  transitionTo: AnimationName[];
}

// Example:
const ANIMATION_GROUPS = {
  conversation: {
    short: ['Talking_0', 'Nod', 'Slight_Gesture'],
    medium: ['Talking_1', 'Talking_2', 'Explaining'],
    long: ['Storytelling', 'Detailed_Explanation']
  },
  emotional: {
    positive: ['Laughing', 'Excited_Jump', 'Happy_Dance'],
    negative: ['Crying', 'Sad_Slump', 'Worried_Fidget'],
    dramatic: ['Surprised_Jump', 'Shocked_Step_Back', 'Angry_Gesture']
  }
};
```

---

## ğŸ”„ **Core Workflows**

### **1. Message Processing Flow**
```
User Input 
    â†“
AI Analysis (Gemini)
    â†“
Emotional Intent + Context
    â†“
Intelligence Layer (Mapping)
    â†“
Expression + Animation Selection
    â†“
Visual Rendering (Avatar)
```

### **2. Expression Selection Algorithm**
```typescript
function selectExpression(emotionalIntent: EmotionalIntent, context: ConversationContext): string {
  // 1. Get base expression group
  const expressionGroup = EXPRESSION_GROUPS[emotionalIntent.primary];
  
  // 2. Apply intensity filter
  const intensityLevel = categorizeIntensity(emotionalIntent.intensity);
  const candidates = expressionGroup.intensityLevels[intensityLevel];
  
  // 3. Apply contextual overrides
  const contextualCandidates = expressionGroup.contextualVariants[context.relationshipTone];
  const finalCandidates = contextualCandidates || candidates;
  
  // 4. Select best match
  return selectOptimalExpression(finalCandidates, context);
}
```

### **3. Animation Coordination Algorithm**
```typescript
function selectAnimation(expression: string, messageData: any, context: ConversationContext): string {
  // 1. Get expression category
  const expressionCategory = getExpressionCategory(expression);
  
  // 2. Consider message characteristics
  const messageLength = messageData.text.length;
  const isQuestion = messageData.text.includes('?');
  const hasEmphasis = /[!]{2,}/.test(messageData.text);
  
  // 3. Select animation group
  const animationGroup = selectAnimationGroup(expressionCategory, messageLength, context);
  
  // 4. Apply special rules
  if (isQuestion) return selectQuestionAnimation(animationGroup);
  if (hasEmphasis) return selectEmphasisAnimation(animationGroup);
  
  return selectDefaultAnimation(animationGroup);
}
```

---

## ğŸ› ï¸ **Implementation Plan**

### **Phase 1: Foundation (2 weeks)**

#### **Week 1: Backend Restructure**
- [ ] Update `ChatResponseDto` Ä‘á»ƒ support emotional intent
- [ ] Modify `AiService` Ä‘á»ƒ output emotional intent thay vÃ¬ specific expression
- [ ] Update Gemini prompt vá»›i emotional intent framework
- [ ] Implement basic emotion validation

#### **Week 2: Frontend Intelligence Layer**
- [ ] Táº¡o `ExpressionMapper` class
- [ ] Implement expression group mappings
- [ ] Basic intensity-based selection logic
- [ ] Update `Avatar` component Ä‘á»ƒ sá»­ dá»¥ng mapping layer

### **Phase 2: Intelligence Enhancement (2 weeks)**

#### **Week 3: Context Analysis**
- [ ] Implement conversation context detection
- [ ] Create contextual override rules
- [ ] Add message type classification
- [ ] User emotional state tracking

#### **Week 4: Animation Coordination**
- [ ] Design animation group system
- [ ] Implement expression-animation compatibility matrix
- [ ] Create transition rules engine
- [ ] Animation sequence coordination

### **Phase 3: Advanced Features (2 weeks)**

#### **Week 5: Mixed Emotions & Transitions**
- [ ] Support secondary emotions
- [ ] Smooth expression transitions
- [ ] Emotion blending for complex states
- [ ] Animation sequence chaining

#### **Week 6: Optimization & Analytics**
- [ ] Performance optimization
- [ ] Expression effectiveness tracking
- [ ] User preference adaptation
- [ ] A/B testing framework

---

## ğŸ“ **File Structure Changes**

### **Backend Changes**
```
src/
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ ai.service.ts              # Modified: Emotional intent logic
â”‚   â”œâ”€â”€ emotion-mapping.ts         # New: Emotion definitions
â”‚   â””â”€â”€ context-analyzer.ts        # New: Context analysis
â”œâ”€â”€ chat/
â”‚   â””â”€â”€ dto/
â”‚       â”œâ”€â”€ chat-response.dto.ts   # Modified: Add emotional intent
â”‚       â””â”€â”€ emotional-intent.dto.ts # New: Intent structure
```

### **Frontend Changes**
```
src/
â”œâ”€â”€ intelligence/
â”‚   â”œâ”€â”€ ExpressionMapper.ts        # New: Core mapping logic
â”‚   â”œâ”€â”€ AnimationCoordinator.ts    # New: Animation selection
â”‚   â”œâ”€â”€ ContextAnalyzer.ts         # New: Context detection
â”‚   â””â”€â”€ TransitionManager.ts       # New: Smooth transitions
â”œâ”€â”€ constants/
â”‚   â”œâ”€â”€ emotion-groups.ts          # New: Expression groupings
â”‚   â”œâ”€â”€ animation-groups.ts        # New: Animation categories
â”‚   â””â”€â”€ mapping-rules.ts           # New: Selection rules
â”œâ”€â”€ components/
â”‚   â””â”€â”€ Avatar.tsx                 # Modified: Use intelligence layer
```

---

## ğŸ¯ **Success Metrics**

### **Phase 1 Success Criteria**
- âœ… AI output 15 emotional intents instead cá»§a 6 expressions
- âœ… Frontend map emotional intent â†’ 60+ expressions
- âœ… No regression trong response quality
- âœ… Expression variety tÄƒng 10x

### **Phase 2 Success Criteria**
- âœ… Context-appropriate expression selection (90%+ accuracy)
- âœ… Animation-expression compatibility (100%)
- âœ… Smooth transitions between expressions
- âœ… User engagement metrics improvement

### **Phase 3 Success Criteria**
- âœ… Complex emotion support (mixed states)
- âœ… User preference adaptation
- âœ… Performance under 100ms for expression selection
- âœ… Expression effectiveness analytics

---

## ğŸš€ **Future Expansion Opportunities**

### **Animation Library Growth**
- **Gesture Animations**: Hand gestures, body language
- **Interaction Animations**: Pointing, offering, receiving
- **Environment Animations**: Looking around, reacting to sounds
- **Micro-Animations**: Breathing, subtle movements, eye movements

### **Advanced Intelligence**
- **User Behavior Learning**: Adapt expressions to user preferences
- **Conversation Memory**: Remember past interactions to influence expressions
- **Multi-modal Input**: Consider voice tone, text sentiment together
- **Real-time Adaptation**: Adjust based on user responses

### **Integration Possibilities**
- **Voice Analysis**: Match expressions vá»›i voice emotion
- **Biometric Integration**: Adapt to user's detected mood
- **Multi-character Support**: Different personalities vá»›i different expression tendencies
- **Customization**: User-defined expression preferences

---

## ğŸ“ **Technical Considerations**

### **Performance Requirements**
- Expression selection: < 50ms
- Animation transition: < 200ms  
- Memory usage: < 100MB additional
- Bundle size impact: < 500KB

### **Compatibility & Fallbacks**
- Graceful degradation náº¿u mapping fails
- Default expression cho unknown emotions
- Animation fallbacks cho missing files
- Browser compatibility cho morph targets

### **Monitoring & Debugging**
- Expression selection logging
- Performance metrics tracking
- User interaction analytics
- Error handling vÃ  recovery

---

## ğŸ”§ **Development Guidelines**

### **Code Quality Standards**
- TypeScript strict mode
- Comprehensive unit tests (>80% coverage)
- Performance benchmarks
- Documentation cho all mapping rules

### **Testing Strategy**
- Unit tests cho mapping logic
- Integration tests cho full workflow
- Visual regression tests cho expressions
- Performance tests cho transition smoothness

### **Deployment Strategy**
- Feature flags cho new expression system
- Gradual rollout vá»›i monitoring
- A/B testing infrastructure
- Rollback plan trong case of issues